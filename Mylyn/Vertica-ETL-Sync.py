#!/opt/vertica/oss/python3/bin/python3

import argparse
import csv
import datetime
import logging
import os
from pprint import pprint

import traceback
import sys

import vertica_python


########################### GLOBAL VARS ###########################

ARGS = None

CONNECTION = None
CURSOR = None

FAILURE_LIST = []
SUCCESS_LIST = []
newtable = 'NativeProd'

KEY_TO_UPDATE = {
    "NativeProdservicecomponentcontainsdevice": ["device_globalid","servicecomponent_globalid"],
    "NativeProdsystemelementcontainsdevice": ["device_globalid","systemelement_globalid"],
    "NativeProdservicecomponentcontainssystemelement": ["systemelement_globalid","servicecomponent_globalid"],
    "NativeProdactualservicecontainsservicecomponent": ["servicecomponent_globalid","actualservice_globalid"],
}

NEW_ROWS_IDENTIFIER = [
    'ADD_RELATION',
    'ADD_ENTITY',    
]

UPDATE_ROW_IDENTIFIER = [
    'UPDATE_ENTITY',     
]

REMOVE_ROW_IDENTIFER = [
    'REMOVE_ENTITY',
    'REMOVE_RELATION'
]

RELATIONSHIP_TABLES = [
    'NativeProdservicecomponentcontainsdevice',
    'NativeProdsystemelementcontainsdevice',
    'NativeProdservicecomponentcontainssystemelement',
    'NativeProdactualservicecontainsservicecomponent',
]

TRANSACTION_TABLES = [
    'comments',
]

###################################################################


def get_args():
    """ Process and Return command line arguments """
    parser = argparse.ArgumentParser(description='Process CSV files and insert to DV')
    parser.add_argument('csv_files_dir', help='Folder Path of CSV Files')
    parser.add_argument('schema', help='schema name')    
    parser.add_argument('db', help='Database name')
    parser.add_argument('db_user', help='DB username')
    parser.add_argument('db_pass', help='DB password')
    parser.add_argument('sync_type', help='Sync Type')
    parser.add_argument('backup_servers', help='list of backup servers')
    return parser.parse_args()


def get_csv_files():
    """
    Returns CSV files in given folder
    """
    try:
        # list files in dir
        files = os.listdir(ARGS.csv_files_dir)
    except FileNotFoundError:
        logging.error(f'No such directory: {ARGS.csv_files_dir}')
        exit(1)
    except Exception as e:
        logging.error(f'An error occurred while listing files from dir: {ARGS.csv_files_dir}')
        logging.error(e)
        logging.error(traceback.format_exc())
        exit(1)
        
    # filter CSV files
    files = list(filter(lambda f: f.endswith('.csv'), files))

    # sort csv files a-z
    files.sort()    

    return files


def get_db_connection():
    conn_info = {
        'host': ARGS.backup_servers.strip().split(',')[0],
        'port': 5433,
        'user': ARGS.db_user,
        'password': ARGS.db_pass,
        'database': ARGS.db,
        # autogenerated session label by default,
        # 'session_label': 'some_label',
        # default throw error on invalid UTF-8 results
        'unicode_error': 'strict',
        # SSL is disabled by default
        'ssl': False,
        # using server-side prepared statements is disabled by default
        'use_prepared_statements': False,
        # connection timeout is not enabled by default
        # 5 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
        'connection_timeout': 10000,
        # 'backup_server_node': ['15.120.115.182', '15.120.115.183']
        'backup_server_node': ARGS.backup_servers.strip().split(','),
    }

    connection  = vertica_python.connect(**conn_info)
    return connection


def create_temp_csv(rows, cols, filename):
    """
    create temp csv file without BI Sync header
    """

    # rename filename
    filename = filename.replace('.csv', '_new.csv')
    file_    = os.path.join('/tmp/NativeProd', filename)

    # headers  = list(rows[0].keys())

    with open(file_, 'w', encoding='utf-8', newline='') as fd:
        writer = csv.DictWriter(fd, fieldnames=cols)
        writer.writeheader()
        for row in rows:
            writer.writerow(dict(row))

    return file_


def execute_sql_command(cmd, filename):
    """
    Exceute DB Command
    """

    logging.info(f'executing command: {cmd}')
    try:
        CURSOR.execute(cmd)
        logging.info(f'Result: {CURSOR.fetchone()[0]}')
        CURSOR.execute('commit')
        return True
    except Exception as e:
        logging.error(f'An error occurred while inserting records from csv {filename}')
        logging.error(traceback.format_exc())        
        FAILURE_LIST.append(filename)
        # exit(1)

def delete_relationship_records():
    """
    delete records from relationship table
    """
    #logging.info(f'deleting records from table {table}')
    table1 = "NativeProdservicecomponentcontainsdevice"
    table2 = "NativeProdsystemelementcontainsdevice"
    table3 = "NativeProdservicecomponentcontainssystemelement"
    table4 = "NativeProdactualservicecontainsservicecomponent"
    cmd1 = f'truncate table {ARGS.schema}.{table1}'
    cmd2 = f'truncate table {ARGS.schema}.{table2}'
    cmd3 = f'truncate table {ARGS.schema}.{table3}'
    cmd4 = f'truncate table {ARGS.schema}.{table4}'
    #return execute_sql_command(cmd, filename)
    CURSOR.execute(cmd1)
    CURSOR.execute(cmd2)
    CURSOR.execute(cmd3)
    CURSOR.execute(cmd4)
    return True

def insert_records(table, columns, csv_file, filename):
    """
    insert records to table
    """
    logging.info(f'inserting records to table {table}')
	
    # construct COPY command to insert data
    columns = ','.join(columns)
    cmd = f"COPY {ARGS.schema}.{table}({columns}) FROM '{csv_file}' PARSER fcsvparser(type='RFC4180', delimiter=',')"
    return execute_sql_command(cmd, filename)


def identify_record_type(rows):
    """
    Identify the records type
    """
    new_and_update_rows = []
    # update_rows = []
    delete_rows = []

    for row in rows:
        if 'BiSyncOperation' in row:
            if row['BiSyncOperation'] in NEW_ROWS_IDENTIFIER:
                new_and_update_rows.append(row)
            elif row['BiSyncOperation'] in UPDATE_ROW_IDENTIFIER:
                new_and_update_rows.append(row)
            elif row['BiSyncOperation'] in REMOVE_ROW_IDENTIFER:
                delete_rows.append(row)
        else:
            new_and_update_rows.append(row)
    return new_and_update_rows, delete_rows


def read_csv(file_):
    """ Reads CSV data and convert to dict"""

    with open(file_, encoding='utf-8') as fd:
        csvreader = csv.DictReader(fd)
        headers = [col.lower() for col in csvreader.fieldnames]
        rows = [row for row in csvreader]

    return rows


def remove_bisync_cols(row):
    if 'RowId' in row:
        row.pop('RowId')
    if 'BiSyncOperation' in row:
        row.pop('BiSyncOperation')
    return row


def get_records_count(table):
    """
    Executes merge command
    """
    
    # construct sql query
    qry = f"SELECT COUNT(*) FROM {ARGS.schema}.{table}"
    
    try:
        CURSOR.execute(qry)
        return CURSOR.fetchone()[0]        
    except Exception as e:
        logging.error(f'An error occurred while executing query: {qry}')
        logging.error(traceback.format_exc())
        FAILURE_LIST.append(filename)        



def initial_sync(csv_file, table):
    """
    Performs initial sync
    """    
    #if delete_relationship_records(table, filename):
        #logging.info(f'Records deleted from relationship table: {table}')
    # logging.info(f'{"#" * 15}  {table}  {"#" * 15}')
    logging.info(f'Identified Table {table}')
    logging.info(f'Reading CSV: {csv_file}')

    # read csv data
    rows = read_csv(csv_file)    
    logging.info(f'{len(rows)} record(s) found')

    if len(rows) > 0:

        # extract filename
        filename = os.path.basename(csv_file)
        if table not in TRANSACTION_TABLES:
            logging.info(f'Removing BI Sync Headers')
            rows = list(map(remove_bisync_cols, rows))

        rows = verify_columns(rows, table, filename)

        columns = list(rows[0].keys())
        columns_lower = [col.lower() for col in columns]

        # create temp csv file
        temp_csv_file = create_temp_csv(rows, columns, filename)
		
        # delete rows
        #if delete_relationship_records(table, filename):
            #logging.info(f'Records deleted from relationship table: {table}')
		
        # insert rows
        if insert_records(table, columns_lower, temp_csv_file, filename):
            SUCCESS_LIST.append(filename)

        logging.info(f'Deleting temp csv_file: {temp_csv_file}')
        os.remove(temp_csv_file)

    else:
        SUCCESS_LIST.append(filename)       


def verify_columns(rows, table, filename):
    """verify the csv columns against table data"""
    cols = get_table_cols(table, filename)
    cols = [col.lower() for col in cols]

    keys = [key.lower() for key in rows[0].keys()]

    logging.info('Verifing columns present in csv file')
    logging.info(f'{len(keys)} columns found in csv and {len(cols)} found in table')    

    if len(cols) == len(keys):
        return rows
    else:
        diff_cols = list(set(cols) - set(keys))
        for row in rows: 
            for col in diff_cols:
                row.update({col: ''})
        
        return rows



def get_table_cols(table, filename):
    """ return list of columns of given table"""
    qry = f"SELECT column_name FROM columns WHERE table_schema = '{ARGS.schema}' AND table_name = '{table}'"

    try:
        CURSOR.execute(qry)
        results = CURSOR.fetchall()
        cols = [row[0] for row in results]        
        # logging.info(f'Result: {cols}')
        return cols
    except Exception as e:
        logging.error(f'An error occurred while executing query: {qry}')
        logging.error(traceback.format_exc())
        FAILURE_LIST.append(filename)
        # exit(1)


def main():
    """
    Main function
    """

    global ARGS, CURSOR, CONNECTION, SUCCESS_LIST, FAILURE_LIST

    # configure log params
    logging.basicConfig(
        # filename='/home/dbadmin/logs/db_sync.log',
        # filemode='a',
        level=logging.DEBUG,
        format="%(asctime)s:%(levelname)s:%(message)s"
    )

    # get command line args
    ARGS = get_args()

    # print(ARGS)

    # set csv file size to sys max
    csv.field_size_limit(sys.maxsize)

    # get csv files
    files = get_csv_files()

    logging.info(f'Total {len(files)} csv files found in dir {ARGS.csv_files_dir}')

    # if no csv files found
    if len(files) == 0:
        exit(0)

    logging.info('Initiating DB connection')
    CONNECTION  = get_db_connection()
    CURSOR      = CONNECTION.cursor()

    all_tables = TRANSACTION_TABLES + RELATIONSHIP_TABLES
    #all_tables = MASTER_TABLES + RELATIONSHIP_TABLES

    delete_relationship_records()
	
    for file_ in files:        
        logging.info(f'Processing File: {file_}')

        # find table
        _last_index = file_.rfind('.')
        table = str(newtable) + file_[:_last_index].lower()
        #table = file_[:_last_index] + str(newtable)
        if 'comment' in table:
           table = 'comments'        
        
        # verify table

        if table is None or table not in all_tables:
            if table is None:
                error = f'unable to find table name from csv file {file_}'
            elif table not in all_tables:
                error = f'Invalid Table {table}, verify the csv file {file_}'

            logging.error(error)
            FAILURE_LIST.append(file_)            

            # continue to next file
            continue

        csv_file = os.path.join(ARGS.csv_files_dir, file_)

        # check sync type
        if ARGS.sync_type.lower() == 'initial' or table == 'comments':
            initial_sync(csv_file, table)
        else:            
            sync_data(csv_file, table)        

    logging.info('Closing DB connection')
    CONNECTION.close()

    logging.info('------SUMMARY-------')    

    logging.info('-----Successfull START----')
    logging.info(f'Successfull CSV list: {len(SUCCESS_LIST)}')

    if len(SUCCESS_LIST) > 0:                
        logging.info('\n'.join(SUCCESS_LIST))

    logging.info('-----Successfull END----')

    logging.info('-----Failure START----')
    logging.info(f'Failure CSV list: {len(FAILURE_LIST)}')
    
    if len(FAILURE_LIST) > 0:                
        logging.info('\n'.join(list(FAILURE_LIST)))
        exit(1)

if __name__ == '__main__':
    main()
